---
title: "Biostat 203B Homework 4"
author: Pablo Geraldo
subtitle: Due Mar 12 @ 11:59PM
output:
  # ioslides_presentation: default
  html_document:
    toc: true
    toc_depth: 4
---

```{r, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```
                      
Display machine information:
```{r}
sessionInfo()
```
Load database libraries and the tidyverse frontend:
```{r}
library(tidyverse)
library(lubridate)
library(miceRanger)
```

## Q1. Missing data

Through the Shiny app developed in HW3, we observe abundant missing values in the MIMIC-IV ICU cohort we created. In this question, we use multiple imputation to obtain a data set without missing values.

0. Read following tutorials on the R package miceRanger for imputation: <https://github.com/farrellday/miceRanger>, <https://cran.r-project.org/web/packages/miceRanger/vignettes/miceAlgorithm.html>.

    A more thorough book treatment of the practical imputation strategies is the book [*_Flexible Imputation of Missing Data_*](https://stefvanbuuren.name/fimd/) by Stef van Buuren. 

1. Explain the jargon MCAR, MAR, and MNAR.

In missing data analysis, it is common to refer to this typology, explained in detail by [Little and Rubin (1987)](https://www.google.com/books/edition/Statistical_Analysis_with_Missing_Data/OaiODwAAQBAJ?hl=en&gbpv=0) "Statistical Analysis with Missing Data". The key point is to identify the reasons why some data is not observed in our dataset and, depending on the process generating missingness, the appropriate way to handle it could change.

**MCAR** (Missing Completely at Random) refers to the case where some values on a variable of interest are unrecorded, but there is no systematic association between that missingness and the observed variables, nor any parameter of interest. This is the same to saying that the complete cases are a random sample of the population of interest, so a complete case analysis would be unbiased.

**MAR** (Missing at Random) refers to the case where we have missing values in a variable of interest, and that missingness *is associated* with the underlying, unobserved values. This means that a complete case analysis would be biased. However, if the process generating missingness is **MAR**, this means that it is *conditionally* at random. In other words, conditional on covariates, what we observed is a random sample of the underlying values. In this case is when multiple imputation, and other imputation methods, actually works, since they impute the missing values based on the values of other, observed, covariates.

**MNAR** (Missing Not at Random), finally, refers to the case where the missingness pattern in a variable of interest is associated to its underlying (unobserved) values, in a way that is not dependent or captures by observed covariates. In this case, there is no way to recover or approximate the values that the variable would have attained have it been completely observed, and therefore no imputation method would do a good job in approximating the full data. Although imputation methods, including multiple imputation, might ameliorate the bias in the complete case analysis, no method would be enough to completely remove bias.

From this brief explanation, we can obtain a few take-aways. First, not all missing data is created equal. Depending on the missingness mechanism, multiple imputation might, or might not, help. Second, that the *missingness mechanism* refers to each variable at a time, not the complete dataset. In other words, we can have in the same dataset variables that are **MCAR** so don't need imputation, others that are **MAR** so we can successfully impute them using a MI algorithm, and variables that are **MNAR**, and therefore no imputation would recover the true distribution. 

Finally, beyond the scope of this explanation but important to mention, is the development of some recent graph-based versions of these categories, that allow researchers to justify their reasoning about missingness using their substantive understanding represented in graphical model. This approach has provided conditioning under which we can address even in the **MNAR** case. More details can be found in [Mohan and Pearl (2021)](https://ftp.cs.ucla.edu/pub/stat_ser/r473-L.pdf).



2. Explain in a couple of sentences how the Multiple Imputation by Chained Equations (MICE) work.

The MICE algorithm works by iterating over a series of predictive models (in this case, random forest) built for each variable at a time, going over all variables to impute, and finish when the algorithm converges:

* Select a variable to impute: `data$v`
* Randomly complete missing entries for all other variables: `data[,-v]`
* Use the `-v` variables as features in the prediction model for `v`
* Iterate across variables until all have been imputed
* Repeat, starting from the imputed dataset, until convergence.

Here, convergence refer to approximating the correlation between variables in the original data. How fast the algorithm converges would depend on how informative the data is; the higher the original correlation, the faster.

Alternatively, instead of the predictive model approach just described, a "predictive mean matching" method can be used. This is just an application of knn to impute missing values, that is most useful to impute multimodal, integer or skewed variables, cases where the predictive approach tend to fail.


3. Perform a data quality check of the ICU stays data. Discard variables with substantial missingness, say >5000 `NA`s. Replace apparent data entry errors by `NA`s.

First, let's start by exploring our dataset. For reference, I print the data before any missing data imputation. Notice that I remove variables uninformative for prediction or imputation (like the identifiers), and any variable that could potentially contain information about the "future" of the patient (like last care unit, time moved out of icu, length of stay, etc.).

```{r, cache=TRUE}
# Load the dataset
path <- "/home/pdgeraldo/biostat-203b-2021-winter/hw3/mimiciv_shiny"
data <- readRDS(paste0(path,"/icu_cohort.rds"))

# Identify variables to remove
vardrop <- c("subject_id", "hadm_id",
             "stay_id", "last_careunit",
             "intime",
             "edregtime","edouttime",
             "outtime","los",
             "dischtime","deathtime",
             "discharge_location",
             "edouttime",
             "hospital_expire_flag",
             "anchor_year","anchor_year_group",
             "dod", "deathdiff")

# Select subset of variables and print the data
data <- data %>% select(-any_of(vardrop))
print(data, width = Inf)
```

Now we have `r nrow(data)` observations and `r ncol(data)` variables. Also, it is important to note that the appropriate way of summarizing variables (and, later, of modeling their missingness) would vary for numeric versus categorical variables, so I would explore them separately. Let's first look at the categorical variables, exploring their distribution by the outcome of interest.

```{r, cache=TRUE}
# Use for some descriptive tables
library("SmartEDA")

# List categorical variables
catvars <- c("mort30",
             "first_careunit",
             "admission_type",
             "admission_location",
             "insurance",
             "language",
             "marital_status",
             "ethnicity",
             "gender")

# Create table for categorical variables
data %>%
  select(all_of(catvars)) %>%
  ExpCTable(Target = "mort30",
            margin=1, clim=10, nlim=3,
            round=2, bin=NULL, per=FALSE)
```

From exploring the categorical variables in our dataset, we can observe a few patterns. First, there are many variables without missing values, or with unspecific but still informative labels. For example, the `language` variable contains "English" and "?" as possibles answers, but we can easily transform this into a binary variable `speaks_english` yes or no. 

In the case of `marital_status`, we have `r nrow(data[data$marital_status=="NA",])` missing values (valid values are Divorced, Married, Single and Widowed). I will use this as as a candidate for imputation, although the amount of missingness might seem a little extreme; I could certainly have processed this variable like `language` creating and "unknown" category, but I'll try to impute it for the sake of the exercise. Finally, we have the `ethnicity` variable, with `r nrow(data[data$ethnicity=="UNKNOWN",])` declared as "unknown", and `r nrow(data[data$ethnicity=="UNABLE TO OBTAIN",])` declared as "unable to obtain". Valid answers are American Indian/Alaska Native, Asian, Black/African American, Hispanic/Latino, White, and Other. Given the amount of missing data, and that I am in doubt if actually imputing the ethnicity would make a lot of sense in an actual application, I will recode both "unknown" and "unable to obtain" as the same category and leave it that way for the analysis.

Let's now look at the numerical variables in our data.

```{r, cache=TRUE}
# List numerical variables
numvars <- data %>%
  # Remove categorical vars
  select(-any_of(catvars)) %>%
  names()

# Summarize numerical variables
data %>% 
  select(all_of(numvars), "mort30") %>%
  # by=G: by group (GA would printi by group and all)
  # gp="mort30": show by outcome
  ExpNumStat(by = "G", gp = "mort30",
             Qnt = seq(0,1,0.25),
             MesofShape = 1,
             Outlier = TRUE,
             round = 2)
```

In the case of numerical variables, we can see a few variables that are problematic, while others seem like good candidates for imputation. On the first group, the `arterial_blood_pressure_*` variables have around 50\% missing, and also many registered negative values, which seems odd. However, notice that  missingness is a around 10\% less in the group that ends up dying within 30 days. My interpretation is that this *invasive* measure is taken only on people already experiencing a complex condition, and having this variable observed makes more likely to end up dying during hospitalization. In this scenario, it is probably not advisable trying to impute the *values* of the variable, but also its presence containes useful predictive information. So I will create an indicator variable to register if the person has a measure of arterial blood pressure or not, and drop its values.

Something similar occurs with `lactate`, that has 42\% missing among those surviving and 19\% missing among those dying. Once again, it is probably useless trying to impute the *values* of that variable for the entire sample; however, it is useful to keep track if there is a valid measure of `lactate` in a patient's record or not, since it is predictive of the outcome. For the rest of the variables, we can see there is a pattern that, in general, patients that end up dying withing the hospitalization tend to have *more* missing values than those surviving. 

```{r, cache=TRUE}
# First, recode the data
data <- 
  data %>%
  mutate(speaks_english = 
           ifelse(language=="ENGLISH", 1, 0),
         # Recode as NA in ethnicity
         ethnicity =
           case_when(
             ethnicity=="UNABLE TO OBTAIN" ~ "NOT REGISTERED",
             ethnicity=="UNKNOWN" ~ "NOT REGISTERED",
             TRUE ~ ethnicity
           ),
         # Indicator for having arterial bp
         arterial_bp_recorded =
           case_when(
             is.na(arterial_blood_pressure_mean) ~ 0L,
             is.na(arterial_blood_pressure_systolic) ~ 0L,
             TRUE ~ 1L
           ),
         # Indicator for lactate
         lactate_recorded = ifelse(is.na(lactate), 0, 1),
         # Transform admittime
         # To have some seasonality information
         # First, time of the day (am vs pm)
         is_am = lubridate::am(admittime) %>%
           as.numeric(),
         # Epidemiological week
         epi_week = lubridate::epiweek(admittime),
         # Epidemiological year
         epi_year = lubridate::epiyear(admittime))

# Remove variables already used
data <- 
  data %>% 
  select(-c("arterial_blood_pressure_mean",
           "arterial_blood_pressure_systolic",
           "lactate",
           "language",
           "admittime"))

# Update our variable listing
# In catvars, replace language by the recoded version
catvars <- c(catvars[1:4],catvars[6:8],
             "speaks_english",
             "arterial_bp_recorded",
             "lactate_recorded",
             "is_am","mort30")
# In numvars, drop used ones and add newly created
numvars <- c(numvars[2:7],numvars[10:19],
             "epi_week","epi_year")
```

We still care about some outliers, as appearing in our exploration (here and in the previous homework). I will use a somewhat common, crude but still useful measure to identify outliers, as $\pm 3 SD$. The reason for this choice is that it is less strict than other alternatives ($\pm 2 SD$, $\pm 1.5\text{ IQR}$), but in the context at hand we would want to allow for some extreme values, while still caughting those that most likely come from errors of measurement. 

```{r, cache=TRUE}
# Now, identify and recode outliers as NA
# For ref: https://www.statology.org/remove-outliers-r/
# For ref: https://www.r-bloggers.com/2017/12/combined-outlier-detection-with-dplyr-and-ruler/

# Calculate z-scores
z_scores <- 
  as.data.frame(
    sapply(data %>% select(numvars),
           function(df) (abs(df-mean(df,na.rm = TRUE))/sd(df,na.rm = TRUE))))

# Use z-scores to recode variables as NA
# Iterate over variables
for(i in numvars){
  # Create an aux variable with z-scores
  aux <- z_scores[,i]
  # Use it to remove NAs
  data[which(aux>=3),i] <- NA
}

# The final (numerical) data looks like this
data %>% 
  select(all_of(numvars)) %>%
  summary()
```

Two final notes on how I am handling missingness. Specially regarding outliers, but true in general, one would want to consult with someone with domain knowledge about plausible values for these health measures, to make sure one is not allowing values to extreme or dropping values that, while being extreme, could still be possible in hospitalized patients. Finally, the previous analysis clearly shows that the information is not **MCAR**, but we can't tell if it is **MAR** from the data. So, to proceed with multiple imputation, we are *assuming* that the data is missing *at random* so we can build the imputation, and later, prediction models.

Now I will recode the data to account for the changes just described. I will also use some of the temporal information (`admittime`) to create variables accounting for seasonality (time of the day, epidemiological week and month). To do so, I will use some functions in the `lubridate` package, including the identification of epidemiological years and weeks.


4. Impute missing values by `miceRanger` (request $m=3$ datasets). This step is very computational intensive. Make sure to save the imputation results as a file.

5. Make imputation diagnostic plots and explain what they mean.

6. Obtain a complete data set by averaging the 3 imputed data sets.

## Q2. Predicting 30-day mortality

Develop at least two analytic approaches for predicting the 30-day mortality of patients admitted to ICU using demographic information (gender, age, marital status, ethnicity), first lab measurements during ICU stay, and first vital measurements during ICU stay. For example, you can use (1) logistic regression (`glm()` function), (2) logistic regression with lasso penalty (glmnet package), (3) random forest (randomForest package), or (4) neural network.

1. Partition data into 80% training set and 20% test set. Stratify partitioning according the 30-day mortality status.

2. Train the models using the training set.

3. Compare model prediction performance on the test set.

